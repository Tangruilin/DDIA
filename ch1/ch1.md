
# 可靠，可拓展并且可维护的应用
> <i>The Internet was done so well that most people think of it as a natural resource like the Pacific Ocean, rather than somthing that was man-made. When was the last time a technology with a scale like that was so error-free
></i>

> ——Alan Kay

如今的大多数应用都是数据密集型而不是计算密集型了。对于这些应用而言，CPU功率很少再能成为他们的限制因素——数据量、数据的复杂度与数据流变化的速率往往是更大的问题。
一个数据密集型应用通常是基于提供通用需求的标准模块构建的。例如，大量的应用系统需要以下模块：
- 数据库：存储数据，以供他们（数据库）或者其他应用可以再次访问；
- 缓存：记录消耗比较大的操作的结果，以加速下一次的访问；
- 索引：允许用户通过关键字搜索数据或是使用各种方式过滤数据；
- 流式处理：采用异步方式向另一个进程发送消息；
- 批处理：定期处理大量积累的数据；

这些或许看上去很稀松平常，这只是因为这些数据系统是相当成功的一个抽象：我们无时无刻不在使用他们，而不用做太多的思考。当需要构建一个应用的时候，大多数工程师不会想着从头开始写一个新的存储引擎，因为数据库就是完成这项工作的一个很好的工具。但是可靠并没有那么简单。有大量的数据系统扮演者不同的角色，因为不同的应用有着不同的需求。这世上有着多种构建缓存的方案，多种构建搜索索引的方案，其他类型的数据系统也有多种构建方案。当构建一个应用的时候，我们仍然要弄清楚那些工具、哪些方法是最适合手头上的任务的。并且当在单个工具完成不了的任务时，将各个工具联合使用时很困难的一件事情。

本书是一场既包含数据系统理论，也包含数据系统实践的旅行，同时还告诉了你如何使用二者去构建一个数据密集型应用。我们将探索各种工具之间有何相同、有何不同、以及他们是如何实现他们的特性的。

在这一章，我们将从我们想要实现的基础开始：可靠、可拓展、可维护的数据系统。我们将说明这意味着什么，简单介绍一些思考他们的方法，并且概览一下我们在之后章节会需要的内容。在接下来的章节，我们会层层递进，看一下工作在数据密集型应用上时需要被考虑到的一些设计方案。

## 认识数据系统

我们通常会将数据库、队列、高数缓存等看作完全不同类别的工具。尽管一个数据库和一个消息队列是有一些高度相似的——有时候都会存储一些数据——但是他们有着非常不同的访问模式，这意味着不同的工作特性，并且因此有非常不同的实现。

所以，为什么我们要将他们归为一类，称之为<i>数据系统</i>呢?

这些年浮现出非常多新的数据库和应用。他们为大量不同的应用场景进行了优化，并且他们不再适合被归类于传统类别。比如，有一些数据库同样被作为消息队列使用（redis），并且也有带有类似于数据库的持久化存储保证功能的消息队列（Apache Kafka）。这些类别之间的边界正在变得越来越模糊。

其次，如今越来越多的应用有了单个组件无法满足所有数据处理和存储需求的广泛要求。因此，工作被分解为可以在单一组件上高效完成的各项任务，并且这些组件会通过应用层代码衔接起来。

举个例子，如果你有一个应用——包含缓存层（使用memcached或者其他工具），或者一个与主数据库独立出来的全文检索服务（例如Elasticsearch或者Solr），通常保持保证缓存层与索引和主数据库的关联是应用层代码的职责。
图1-1给出了这个过程的一个概览（我们会在后续章节涉及更多细节）：
<div align="center"><img src="https://raw.githubusercontent.com/Tangruilin/GithubImages/main/img/20211020142743.png" alt = "Figure1-1"/></div>
<center>图1-1包含数个组件的数据系统可能架构</center>

当你结合多个组件来提供服务的时候，服务接口或者说API通常会为客户隐藏这些实现细节。这样我们就基于更小的，相对通用的组件从根本上构建了一个新的，更有针对性的数据系统。你混合构造的数据系统应该提供一下几个具体的保证：缓存应该在写入的时候正确地刷新以让外部用户可以看到一致性的结果。你现在不仅仅是一个应用构建者，同时也是一个数据系统构建者了。

如果你在设计一个数据系统或者服务，一大堆棘手的问题会冒出来。即使是在系统发生局部性错误的时候，你应该如何保证数据的正确性和完整性？即使是在系统的部分发生降级的时候，你如何向客户保证一致性？当负载提高的时候你应该如何拓展？一个好的服务API是什么样的？

有很多可以影响一个数据系统的设计的因素，包含了从业人员的技巧和经验，遗留的系统依赖，交付周期，你的团队对不同的错误的容忍程度，监管规范等。这些因素与环境的关系非常大。

在这本书中，我们会聚焦于对大多数软件系统最重要的三个点：
- 可靠性

    即使是在遇到故障（硬件错误或者软件错误，甚至是人为错误）时，系统都应该正确地工作（在所期待的水平表现出正确的行为）。

- 可拓展性

    在系统需要拓展时（数据层面，流量层面，或者复杂度），处理这些拓展的方式是要有依据的。

- 可维护性

    随着时间的推移，会有很多不同的人在这个系统上工作（工程师或者操作人员，都会维护一些当前的工作或者为新的情景去改造系统），他们应该能在系统上有所产出。

这些词汇经常被翻来覆去地思考，但是很多人对他们没有一个好的理解。为了那些乐于思考的工程师们的利益，我们将会用这个章节剩下的部分思考可靠性，可拓展性，可维护性。接下来，在剩下的章节，我们会看各种各样被用来实现这些目标的技术、架构、以及算法。

## 可靠性
关于什么是可靠和不可靠，每个人都会有一个直觉上的印象。对于软件来说，可靠性会包含以下几点：
- 应用可以表现出使用者期望的功能；
- 应用可以容忍使用者犯错，或是以非期望的方式使用软件；
- 在预料中的负载以及数据量的情况下，对于所要求的使用情形有足够好的表现；
- 系统会对所有未经授权的访问以及滥用进行阻止；
如果说这些要求加在一起称为“正确工作”，那我们可以将可靠性粗略地理解为，“即使是在有些事情出错的时候，也能持续正确地工作”。

我们将可以导致错误的事物称为缺陷(faults)， 可以预知缺陷并且能对付他们的系统称为容错和弹性。前者有一定的误导性：他认为我们可以构建一个可以容忍各种缺陷的系统，而这在现实世界中是不可能的。如果整个地球（包括所有的服务）都被一个黑洞吸入，容错需要在太空中进行网络托管——祝你好运，希望你的预算可以通过。因此我们只谈论具体类型缺陷的容错。

要注意缺陷和失败(failure)是不一样的。缺陷通常被定义为系统的一个组件偏离了原有的功能，系统完全停止服务才被称为失败（或者可以叫做宕机）。将缺陷的几率降低到0是有可能的；因此最好是设计一套高容错机制来防止缺陷导致的宕机。本书中，我们涉及了多种技术来从不可靠的部分中构建可靠的系统。

违反直觉的是，在容错系统中，可以通过故意触发的方式来增加缺陷的比率——例如，在不告警的情况下随机kill掉一些进程。很多关键的bug往往来自于过于薄弱的错误处理；通过刻意引入缺陷，你可以确保容错机制持续受到锻炼与测试，这可以增强你对于缺陷在自然情形下发生时被正确处理的信心。*Netflix Chaos Monkey*就是这一方案的一个典型例子。

尽管我们通常更倾向于容错而不是消灭缺陷了，但还是有些时候消灭缺陷比容错是更好的（因为有些情况没法做容错）。这里有一个安全相关的场景，例如：如果一个攻击者入侵了一个系统并且获取了敏感数据的权限，这个时间是没法撤销的。然而，这本书主要是讨论那些可以容错的缺陷，这些会在接下来的几个部分讨论。

## 硬件错误
当我们寻找系统失败的原因的时候，硬件错误是最先被想到的。硬盘挂了，RAM出错，停电了，网线被某人拔掉了。任何一个工作在大型数据中心的人都能告诉你，当你有很多机器的时候，这些事情无时无刻不在发生。据报告，硬盘的平均使用寿命在10到50年。因此，在一个有着10000个硬盘的存储集群里面，我们预估每个硬盘应该每年损坏一次。

我们的第一方案是，为每一个单个硬件组件的错误增加冗余，从而减少系统失败的概率,CPU应该有多个供电源或者热拔插，并且数据中心应该有着电池或者柴油发电机作为备用电源。当一个组件挂掉的时候，冗余的组件应该替代他的工作直到这个组件被替换掉。这种方案没办法完全组织系统失败，但是这很容易理解并且经常能让一台机器不宕机跑数年。

直到现在，硬件冗余对于大多数的应用来说是足够的，因为他使得在一台机器上发生问题的概率变得微乎其微。只要你能快速地将备份转移到新的机器上，对大多数应用来说，在失败的时候停机一会儿并不是灾难性的。因此，多机冗余只在少数的对高可用有必要要求的应用上才有需求。

然而，随着数据规模和应用程序计算需求的增加，越来越多的应用开始使用更多的机器，这个在概率上提升了硬件缺陷发生的比率。 更重要的是，在类似于AWS等的云平台上，虚拟机在没有告警的情况下变得不可用变得很平常，因为这些平台设计的弹性和灵活性是基于单机可靠性上的。

因此需要有一种可以接受整机丢失的系统，这可以靠增加更完美的软件容错或者增加硬件冗余来实现。这种系统一样有着一个操作上的优先级：如果你需要重启一个单机系统，你要计划一个关机时间（例如，上传一个操作系统的安全补丁），然而一个能容忍机器失败的系统可以在一个时刻对某一个节点上传补丁，而不需要整个系统关机。

## 软件错误
通常我们认为硬件错误是随机的并且不依赖于任何其他设施：一台机器的磁盘损坏并不意味着另一台机器的磁盘损坏。他们之间具有弱相关性（举一个具有相同出错原因的例子，比如由于服务处的温度），但是因此也不太可能出现同一时间大量的硬件错误的情况。

另一类缺陷是系统中的系统性错误。这类缺陷更难去反制，并且由于他们是与节点相关的，他们倾向于比硬件缺陷制造更多的系统失败。
例子如下：
- 当给出特定的坏的输入的时候，一个软件错误会使得一个应用服务的每一个部分都宕机。
比如，可以考虑一下在2012年6月30日的闰秒时间，由于linux内核的bug，这一事件导致了大量的应用挂起。
- 一个跑飞了线程使用了一些共享资源——CPU时间，内存，磁盘空间，或者网络带宽。
- 系统所依赖的服务速度变慢，变得不在回应，或者开始返回错误的响应。
- 级联错误，一个小组件的错误触发了另一个组件错误，另一个组件错误又触发了更多的错误；

造成这些软件缺陷的bug通常会在他们被一系列的不同寻常的操作触发前呆很久。在这些不同寻常的操作中，表明软件正在对环境做出一些假设——虽然这些假设往往是正确的，软件最终还是因为某些原因停下来了。

对于软件上的系统缺陷没有快速的解决办法。很多小的方法可以有所帮助：仔细地对系统的假设以及相互作用进行考虑；进行测试；进程隔离；允许进程宕机和重启；测量，监控，并且分析生产过程中的系统行为。如果一个系统需要做出某项保证（例如一个需要保证写入的消息和读出的消息数目相同的消息队列），可以持续在运行时持续检查，并且在发现纰漏的时候发出告警。

## 人为错误

人们设计并且修建了软件系统，同时保持系统一直运行的操作员也是人。即使是有着最好的想法，人也是不可靠的。例如，一个大型互联网服务调查发现操作人员的配置错误是系统宕机的主要因素，硬件错误（服务或者网络）在宕机中产生影响的情况只有10%~25%。

那么如何让我们的系统变得可靠呢，尽管人是不可靠的？最好的系统集合了一下几种方案：

- 用一种出错概率最小的方案设计系统。例如，好的抽象，API，以及管理接口可以比较容易地“做正确的事情”并且阻拦“做错误的事情”。然而，如果接口过于严格，人们将会绕开他们，否认他们的好处，因此需要一种微妙的平衡来实现正确。

- 将人们经常犯错的地方和人们容易造成错误的地方分开。尤其是，提供一个功能齐全的非生产的环境，这里人们可以安全地探索环境，使用真实的数据，不影响实际的使用者。

- 做任何级别的严格测试，从单元测试到系统集成测试到手工测试。自动化测试现在被广泛使用并且很好地理解了，在常规的测试中无法覆盖到的边角情况中尤有价值。

- 允许从人为错误中快速并且轻松恢复，来从一个错误情况下最小化影响。例如，使设置变化的回滚可以很快进行，逐步增加新代码（因此任何不在期望中的bug只会影响一小部分人），并且提供重新计算的工具（以防出现旧的计算结果出错的情况）。

- 建立起详细并且清晰的监控，比如性能检测和错误比例。在其他的工程学科中，这杯称为遥测（$telemetry$）。（当火箭离开地面以后，遥测对于轨迹上发生了什么以及理解发生的错误有相当重要的作用）。监控可以对我们发出告警信号，并且允许我们检查是否有哪些假设以及月数被触犯到了。当问题发生后，指标在诊断问题的时候是十分重要的。

- 实现好的管理培训和训练——一个复杂的层面，在本书的后面有所提及。

## 稳定性有多重要

稳定性不只是核能设施以及空中交通控制系统需要考虑的——更多的普通应用也需要稳定工作。商业系统的Bug会导致生产力的下降（如果报告了错误的结果还会有法律风险），并且商业网站的中断在经济上和信用上都会造成很大的损失。

即使是在“不严格”应用中，我们也对我们的使用者有一份责任。想一下，一堆父母将他们还是的照片和视频都保存在了你的照片应用中。如果数据突然丢失了他们会有什么感受？他们会知道如何将这些恢复吗？

有一些场景我们应该为了节省开发成本而选择较低的可靠性（当我们开发一个原型产品的时候）或者运营成本（对于一个利润微薄的服务）——但是我们需要非常明确什么时候我们可以偷工减料。

## 可拓展性

